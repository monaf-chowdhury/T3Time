{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5526e3",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82396a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time, os, random, faulthandler, torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from data_provider.data_loader_emb import Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom\n",
    "from models.T3Time import TriModal\n",
    "from utils.metrics import MSE, MAE, metric\n",
    "faulthandler.enable()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:150\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e8807",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        scaler,\n",
    "        channel,\n",
    "        num_nodes,\n",
    "        seq_len,\n",
    "        pred_len,\n",
    "        dropout_n,\n",
    "        d_llm,\n",
    "        e_layer,\n",
    "        d_layer,\n",
    "        head,\n",
    "        lrate,\n",
    "        wdecay,\n",
    "        device,\n",
    "        epochs\n",
    "    ):\n",
    "        self.model = TriModal(\n",
    "            device=device, channel=channel, num_nodes=num_nodes, seq_len=seq_len, pred_len=pred_len, \n",
    "            dropout_n=dropout_n, d_llm=d_llm, e_layer=e_layer, d_layer=d_layer, head=head\n",
    "        )\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=min(epochs, 50), eta_min=1e-6)\n",
    "        self.loss = MSE\n",
    "        self.MAE = MAE\n",
    "        self.clip = 5\n",
    "        print(\"The number of trainable parameters: {}\".format(self.model.count_trainable_params()))\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "\n",
    "\n",
    "    def train(self, input, mark, embeddings, real):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        predict = self.model(input, mark, embeddings)\n",
    "        loss = self.loss(predict, real)\n",
    "        loss.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "        self.optimizer.step()\n",
    "        mae = self.MAE(predict, real)\n",
    "        return loss.item(), mae.item()\n",
    "    \n",
    "    def eval(self, input, mark, embeddings, real_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predict = self.model(input,mark, embeddings)\n",
    "        loss = self.loss(predict, real_val)\n",
    "        mae = self.MAE(predict, real_val)\n",
    "        return loss.item(), mae.item()\n",
    "\n",
    "def load_data(args):\n",
    "    data_map = {\n",
    "        'ETTh1': Dataset_ETT_hour,\n",
    "        'ETTh2': Dataset_ETT_hour,\n",
    "        'ETTm1': Dataset_ETT_minute,\n",
    "        'ETTm2': Dataset_ETT_minute\n",
    "    }\n",
    "    data_class = data_map.get(args.data_path, Dataset_Custom)\n",
    "    train_set = data_class(flag='train', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    val_set = data_class(flag='val', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    test_set = data_class(flag='test', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "\n",
    "    scaler = train_set.scaler\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "\n",
    "    return train_set, val_set, test_set, train_loader, val_loader, test_loader, scaler\n",
    "\n",
    "def seed_it(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"OMP_NUM_THREADS\"]  = \"1\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"]  = \"1\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark   = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08faf1",
   "metadata": {},
   "source": [
    "# Few shot forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82457e89",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    device='cuda', data_path='ETTm1',\n",
    "    channel=128, batch_size=64, dropout_n=0.5,\n",
    "    e_layer=1, d_layer=3,\n",
    "    num_nodes=7, seq_len=512, pred_len=96,\n",
    "    learning_rate=1e-3, d_llm=768,\n",
    "    head=8, weight_decay=1e-3,\n",
    "    num_workers=10, model_name='gpt2',\n",
    "    epochs=40, seed=2024,\n",
    "    es_patience=10, save='./logs/custom-save-path-'\n",
    ")\n",
    "\n",
    "seed_everything(args.seed)\n",
    "g1 = torch.Generator().manual_seed(args.seed)\n",
    "percent = 0.1                                                       # Percent of data for few shot forecasting\n",
    "\n",
    "# ------------------------------------------Few Shot DataLoading----------------------\n",
    "t1 = time.time()\n",
    "train_set, val_set, test_set, train_loader, val_loader, test_loader, scaler = load_data(args)\n",
    "\n",
    "\n",
    "num_train       = len(train_set)\n",
    "indices         = list(range(num_train))\n",
    "few_shot_size   = int(num_train * percent)                          # Size of the few shot. e.g 10%\n",
    "rest_size       = num_train - few_shot_size\n",
    "last_indices    = list(range(num_train - few_shot_size, num_train))\n",
    "\n",
    "first_subset    = Subset(train_set, indices[:few_shot_size])        # taking the first 10% \n",
    "last_subset     = Subset(train_set, last_indices)                   # taking the last 10% \n",
    "random_set, _   = random_split(train_set, [few_shot_size, rest_size], generator=g1) \n",
    "\n",
    "\"\"\"\n",
    "train_loader    = DataLoader(first_subset, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)   # First subset\n",
    "train_loader    = DataLoader(last_subset, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)    # Last subset\n",
    "train_loader    = DataLoader(random_set, batch_size=args.batch_size, shuffle=True, drop_last=True, num_workers=args.num_workers)      # Random subset\n",
    "\"\"\"\n",
    "\n",
    "g2 = torch.Generator().manual_seed(args.seed)\n",
    "train_loader = DataLoader(random_set, batch_size = args.batch_size, shuffle = True, drop_last = True, num_workers = args.num_workers,\n",
    "    generator = g2, worker_init_fn = lambda worker_id: np.random.seed(args.seed + worker_id))\n",
    "\n",
    "\n",
    "print(f\"Data ready for few shot learning\")\n",
    "# ------------------------------------------- Training  ------------------------------\n",
    "\n",
    "print()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loss = 9999999\n",
    "test_log = 999999\n",
    "epochs_since_best_mse = 0\n",
    "\n",
    "path = os.path.join(args.save, args.data_path, \n",
    "                    f\"{args.pred_len}_{args.channel}_{args.e_layer}_{args.d_layer}_{args.learning_rate}_{args.dropout_n}_{args.seed}/\")\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "his_train_loss = []\n",
    "his_train_mae = []\n",
    "his_loss = []\n",
    "his_loss_mae = []\n",
    "val_time = []\n",
    "train_time = []\n",
    "print(args)\n",
    "\n",
    "engine = trainer(\n",
    "    scaler=scaler,\n",
    "    channel=args.channel,\n",
    "    num_nodes=args.num_nodes,\n",
    "    seq_len=args.seq_len,\n",
    "    pred_len=args.pred_len,\n",
    "    dropout_n=args.dropout_n,\n",
    "    d_llm=args.d_llm,\n",
    "    e_layer=args.e_layer,\n",
    "    d_layer=args.d_layer,\n",
    "    head=args.head,\n",
    "    lrate=args.learning_rate,\n",
    "    wdecay=args.weight_decay,\n",
    "    device=device,\n",
    "    epochs=args.epochs\n",
    ")\n",
    "\n",
    "print(\"Start training...\", flush=True)\n",
    "\n",
    "for i in range(1, args.epochs + 1):\n",
    "    t1 = time.time()\n",
    "    train_loss = []\n",
    "    train_mae = []\n",
    "    \n",
    "    for iter, (x,y,x_mark,y_mark, embeddings) in enumerate(train_loader):\n",
    "        trainx = torch.Tensor(x).to(device) # [B, L, N]\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        trainx_mark = torch.Tensor(x_mark).to(device) \n",
    "        train_embedding = torch.Tensor(embeddings).to(device)\n",
    "        metrics = engine.train(trainx, trainx_mark, train_embedding, trainy)\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mae.append(metrics[1])\n",
    "\n",
    "    t2 = time.time()\n",
    "    log = \"Epoch: {:03d}, Training Time: {:.4f} secs\"\n",
    "    print(log.format(i, (t2 - t1)))\n",
    "    train_time.append(t2 - t1)\n",
    "\n",
    "    # validation\n",
    "    val_loss = []\n",
    "    val_mae = []\n",
    "    s1 = time.time()\n",
    "\n",
    "    for iter, (x,y,x_mark,y_mark, embeddings) in enumerate(val_loader):\n",
    "        valx = torch.Tensor(x).to(device)\n",
    "        valy = torch.Tensor(y).to(device)\n",
    "        valx_mark = torch.Tensor(x_mark).to(device)\n",
    "        val_embedding = torch.Tensor(embeddings).to(device)\n",
    "        metrics = engine.eval(valx, valx_mark, val_embedding, valy)\n",
    "        val_loss.append(metrics[0])\n",
    "        val_mae.append(metrics[1])\n",
    "\n",
    "    s2 = time.time()\n",
    "    log = \"Epoch: {:03d}, Validation Time: {:.4f} secs\"\n",
    "    print(log.format(i, (s2 - s1)))\n",
    "    val_time.append(s2 - s1)\n",
    "\n",
    "    mtrain_loss = np.mean(train_loss)\n",
    "    mtrain_mae = np.mean(train_mae)\n",
    "    mvalid_loss = np.mean(val_loss)\n",
    "    mvalid_mae = np.mean(val_mae)\n",
    "\n",
    "    his_train_loss.append(mtrain_loss)\n",
    "    his_train_mae.append(mtrain_mae)\n",
    "    his_loss.append(mvalid_loss)\n",
    "    his_loss_mae.append(mvalid_mae)\n",
    "\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    log = \"Epoch: {:03d}, Train Loss: {:.4f}, Train MAE: {:.4f} \"\n",
    "    print(\n",
    "        log.format(i, mtrain_loss, mtrain_mae),\n",
    "        flush=True,\n",
    "    )\n",
    "    log = \"Epoch: {:03d}, Valid Loss: {:.4f}, Valid MAE: {:.4f}\"\n",
    "    print(\n",
    "        log.format(i, mvalid_loss, mvalid_mae),\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    if mvalid_loss < loss:\n",
    "        print(\"###Update tasks appear###\")\n",
    "        if i <= 10:\n",
    "            \n",
    "            loss = mvalid_loss\n",
    "            torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "            bestid = i\n",
    "            epochs_since_best_mse = 0\n",
    "            print(\"Updating! Valid Loss:{:.4f}\".format(mvalid_loss), end=\", \")\n",
    "            print(\"epoch: \", i)\n",
    "        else:\n",
    "            test_outputs = []\n",
    "            test_y = []\n",
    "\n",
    "            for iter, (x,y,x_mark,y_mark, embeddings) in enumerate(test_loader):\n",
    "                testx = torch.Tensor(x).to(device)\n",
    "                testy = torch.Tensor(y).to(device)\n",
    "                testx_mark = torch.Tensor(x_mark).to(device)\n",
    "                test_embedding = torch.Tensor(embeddings).to(device)\n",
    "                with torch.no_grad():\n",
    "                    preds = engine.model(testx, testx_mark, test_embedding)\n",
    "                test_outputs.append(preds)\n",
    "                test_y.append(testy)\n",
    "            \n",
    "            test_pre = torch.cat(test_outputs, dim=0)\n",
    "            test_real = torch.cat(test_y, dim=0)\n",
    "\n",
    "            amse = []\n",
    "            amae = []\n",
    "            \n",
    "            for j in range(args.pred_len):\n",
    "                pred = test_pre[:, j,].to(device)\n",
    "                real = test_real[:, j, ].to(device)\n",
    "                metrics = metric(pred, real)\n",
    "                log = \"Evaluate best model on test data for horizon {:d}, Test MSE: {:.4f}, Test MAE: {:.4f}\"\n",
    "                amse.append(metrics[0])\n",
    "                amae.append(metrics[1])\n",
    "\n",
    "            log = \"On average horizons, Test MSE: {:.4f}, Test MAE: {:.4f}\"\n",
    "            print(\n",
    "                log.format(\n",
    "                    np.mean(amse), np.mean(amae)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if np.mean(amse) < test_log:\n",
    "                test_log = np.mean(amse)\n",
    "                loss = mvalid_loss\n",
    "                torch.save(engine.model.state_dict(), path + \"best_model.pth\")\n",
    "                epochs_since_best_mse = 0\n",
    "                print(\"Test low! Updating! Test Loss: {:.4f}\".format(np.mean(amse)), end=\", \")\n",
    "                print(\"Test low! Updating! Valid Loss: {:.4f}\".format(mvalid_loss), end=\", \")\n",
    "\n",
    "                bestid = i\n",
    "                print(\"epoch: \", i)\n",
    "            else:\n",
    "                epochs_since_best_mse += 1\n",
    "                print(\"No update\")\n",
    "\n",
    "    else:\n",
    "        epochs_since_best_mse += 1\n",
    "        print(\"No update\")\n",
    "\n",
    "    engine.scheduler.step()\n",
    "\n",
    "    if epochs_since_best_mse >= args.es_patience and i >= args.epochs//2: # early stop\n",
    "        break\n",
    "\n",
    "# Output consumption\n",
    "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "print(\"Average Validation Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "# Test\n",
    "print(\"Training ends\")\n",
    "print(\"The epoch of the best result：\", bestid)\n",
    "print(\"The valid loss of the best model\", str(round(his_loss[bestid - 1], 4)))\n",
    "\n",
    "engine.model.load_state_dict(torch.load(path + \"best_model.pth\"))\n",
    "\n",
    "test_outputs = []\n",
    "test_y = []\n",
    "\n",
    "for iter, (x,y,x_mark,y_mark, embeddings) in enumerate(test_loader):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    testy = torch.Tensor(y).to(device)\n",
    "    testx_mark = torch.Tensor(x_mark).to(device)\n",
    "    test_embedding = torch.Tensor(embeddings).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = engine.model(testx, testx_mark, test_embedding)\n",
    "    test_outputs.append(preds)\n",
    "    test_y.append(testy)\n",
    "\n",
    "test_pre = torch.cat(test_outputs, dim=0)\n",
    "test_real = torch.cat(test_y, dim=0)\n",
    "\n",
    "amse = []\n",
    "amae = []\n",
    "\n",
    "for j in range(args.pred_len):\n",
    "    pred = test_pre[:, j,].to(device)\n",
    "    real = test_real[:, j, ].to(device)\n",
    "    metrics = metric(pred, real)\n",
    "    log = \"Evaluate best model on test data for horizon {:d}, Test MSE: {:.4f}, Test MAE: {:.4f}\"\n",
    "    amse.append(metrics[0])\n",
    "    amae.append(metrics[1])\n",
    "\n",
    "log = \"On average horizons, Test MSE: {:.4f}, Test MAE: {:.4f}\"\n",
    "print(log.format(np.mean(amse), np.mean(amae)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeCMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
